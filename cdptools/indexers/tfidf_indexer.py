#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
import math
from collections import Counter
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from typing import Dict, List, NamedTuple, Union

from .indexer import Indexer

###############################################################################

log = logging.getLogger(__name__)

###############################################################################


class DocumentDetails(NamedTuple):
    unique_id: str
    path: Path


class TFIDFIndexer(Indexer):

    def __init__(self, max_synchronous_jobs: int = 8, **kwargs):
        # Set state
        self.n_workers = max_synchronous_jobs

    def _generate_term_counts_for_document(transcript_details: DocumentDetails) -> Dict[str, Dict[str, float]]:
        # Open the transcript and get raw
        raw = Indexer.get_raw_transcript(transcript_details.path)

        # Clean the transcript text
        cleaned = Indexer.clean_text_for_indexing(raw)

        # Count terms
        # Thanks python core modules :^)
        terms = cleaned.split(" ")
        return {"unique_id": transcript_details.unique_id, "term_counts": Counter(terms)}

    @staticmethod
    def _combine_term_counts_for_corpus(
        corpus_results: List[Dict[str, Union[DocumentDetails, Counter]]]
    ) -> Dict[str, Dict[str, int]]:
        # Go through each term count dictionary and combine into single
        combined = {}
        for id_term_counts in corpus_results:
            # For each term count Counter returned get each term and it's count
            for term, count in id_term_counts["term_counts"].items():
                # If the term has been seen before, simply add a new entry for the term count for that unique id
                if term in combined:
                    combined[term][id_term_counts["unique_id"]] = count
                # If the term hasn't been seen, create a new dictionary to store term count by unique id
                else:
                    combined[term] = {id_term_counts["unique_id"]: count}

        return combined

    @staticmethod
    def _calculate_tfidf_for_corpus(term_counts_corpus: Dict[str, Dict[str, int]]) -> Dict[str, Dict[str, float]]:
        # N: total number of documents in the corpus
        # Let's use sets to create a list of all unique documents in the term counts corpus
        corpus = set()
        for term in term_counts_corpus:
            # Do set union to continue to add unique items to the corpus map
            documents = set(term_counts_corpus[term].keys())
            corpus = corpus.union(documents)

        # N is just now the length of the corpus
        N = len(corpus)

        # Compute tfidf values for the entire corpus
        tfidf_corpus = {}
        for term in term_counts_corpus:
            for unique_id, tf in term_counts_corpus[term].items():
                # Compute idf
                # d_t: documents containing term
                # The number of documents containing a term is just the length of the available keys for that term
                # in the term counts corpus
                # Ex:
                # "hello": {
                #   "0000...": 3,
                #   "1111...": 7,
                #   "9999...": 2
                # }
                # The term "hello" occurs in 3 documents in the above example
                d_t = len(term_counts_corpus[term])
                idf = math.log(N / d_t)

                # If term already seen, simply add a new entry for the term tfidf value for that unique id
                if term in tfidf_corpus:
                    tfidf_corpus[term][unique_id] = tf * idf
                # If the term hasn't been seen, create a new dictionary to store term tfidf value by unique id
                else:
                    tfidf_corpus[term] = {unique_id: tf * idf}

        return tfidf_corpus

    def generate_index(self, document_corpus_map: Dict[str, Path]) -> Dict[str, Dict[str, float]]:
        # Convert the document corpus map to a list of TranscriptDetails for easier passing to multiprocessing
        document_corpus_map = [DocumentDetails(unique_id, path) for unique_id, path in document_corpus_map.items()]

        # We could use sklearn CountVectorizer/ TfidfVectorizer, but meh ðŸ¤·
        # I don't think loading all the files into memory and stuffing them into a single list is that great
        # These transcripts can get long....
        # Memory shouldn't be an issue now, but maybe at like ~4000 documents?
        with ProcessPoolExecutor(max_workers=self.n_workers) as exe:
            results = list(exe.map(TFIDFIndexer._generate_term_counts_for_document, document_corpus_map))

        # Combine the single unique id results
        results = self._combine_term_counts_for_corpus(results)

        # Compute tfidf values for the combined results
        index = self._calculate_tfidf_for_corpus(results)

        return index
